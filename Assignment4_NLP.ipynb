{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4-RC_Daniel-Rotem-Hadar",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-WJBimYDLJS"
      },
      "source": [
        "# Assignment 4\n",
        "Training a simple neural net for relation classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgBKQyWimaRb",
        "outputId": "48db8214-5369-494e-c1e9-be9607911f2a"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3enPCGBF8FlX"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import warnings\n",
        "import itertools\n",
        "from transformers import BertTokenizer\n",
        "from transformers import logging\n",
        "from transformers import BertForSequenceClassification\n",
        "from statistics import mean\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "import numpy as np \n",
        "import random\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from collections import namedtuple\n",
        "import pandas as pd\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "logging.set_verbosity_error()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "warnings.filterwarnings('ignore')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5QSIEoyDdWh"
      },
      "source": [
        "In this assignment you are required to build a full training and testing pipeline for a neural relation classification (RC), using BERT.\n",
        "\n",
        "The dataset that you will be working on is called SemEval Task 8 dataset (https://arxiv.org/pdf/1911.10422v1.pdf). The dataset contain only train and test split, but you are allowed to split the train dataset into dev if needed.\n",
        "\n",
        "\n",
        "The two files (train and test) are available from the course git repository (https://github.com/kfirbar/nlp-course)\n",
        "\n",
        "\n",
        "In this work we will use the hugingface framework for transformers training and inference. We recomand reading the documentation in https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification *before* you start coding. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ul2Y3vuPoV8"
      },
      "source": [
        "**Task 1:** Write a funtion *read_data* for reading the data from a single file (either train or test). This function recieves a filepath and returns a list of sentence. Every sentence is encoded as a touple, where the first element is the sentence string and the second the label (also represented as a sting). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43UEzjFF2K8O"
      },
      "source": [
        "# I built this by looking at the data. Is there a better way? we won't always have a data to look at.. \n",
        "tag_set = {\n",
        "  'Component-Whole',\n",
        "  'Content-Container',\n",
        "  'Cause-Effect',\n",
        "  'Entity-Destination',\n",
        "  'Entity-Origin',\n",
        "  'Instrument-Agency',\n",
        "  'Member-Collection',\n",
        "  'Message-Topic',\n",
        "  'Product-Producer', \n",
        "  'Other'\n",
        "}\n",
        "\n",
        "def get_tag_for(sentence):\n",
        "  for key in tag_set:\n",
        "    if key in sentence:\n",
        "      return key\n",
        "  return 'Other'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1ru5OwpzSFr"
      },
      "source": [
        "def transform_data(data):\n",
        "  # The '\\r' replacement are for cases where the data is acting odd, \n",
        "  # for instance when reading sentence 627 from the train-file \n",
        "  # \"The <e1>sail</e1> had an important <e2>roach</e2> and had adjustable \n",
        "  # batten fittings on the leech and luff receptacles.\" for some kind of reason we get \n",
        "  # '<e2>roach<       /e2>'.\n",
        "  return data.replace('\\r', '').split('\\n\\n') \n",
        "\n",
        "def get_sentence_and_tag(complex_sentence):\n",
        "  sentence_split = complex_sentence.split('\\n')\n",
        "  line, sentence = sentence_split[0].split('\\t')\n",
        "  sentence = sentence.strip('\"')\n",
        "  tag = get_tag_for(sentence_split[1])\n",
        "  return sentence, tag\n",
        "\n",
        "def read_data(filepath):\n",
        "  data = []\n",
        "  with open(filepath, \"r\") as file:\n",
        "    fdata = file.read()\n",
        "    fdata = transform_data(fdata)\n",
        "    fdata = fdata[:-1] # The last object will be empty so we want to ignore it\n",
        "    for complex_sentence in fdata:\n",
        "      sentence, tag = get_sentence_and_tag(complex_sentence)\n",
        "      data.append((sentence, tag))\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2x9joOnzWFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87021dff-8d31-49bf-85cd-0ac97fe41b78"
      },
      "source": [
        "!git clone https://github.com/kfirbar/nlp-course"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'nlp-course' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOpes5kGE66R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f1e16de-6741-4781-bb00-69871677b1c5"
      },
      "source": [
        "# train\n",
        "train = read_data('/content/nlp-course/TRAIN_FILE.TXT')\n",
        "idx = 47\n",
        "print(\"Data size:\", len(train))\n",
        "print(train[idx][0])\n",
        "print(train[idx][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data size: 8000\n",
            "As in the popular movie \"Deep Impact\", the action of the Perseid <e1>meteor shower</e1> is caused by a <e2>comet</e2>, in this case periodic comet Swift-Tuttle.\n",
            "Cause-Effect\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F31OxUpE-HJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feab2ff1-7780-4a36-8469-cdac0773ce89"
      },
      "source": [
        "# test\n",
        "test = read_data('/content/nlp-course/TEST_FILE_FULL.TXT')\n",
        "idx = 0\n",
        "print(\"Data size:\", len(test))\n",
        "print(test[idx][0])\n",
        "print(test[idx][1])\n",
        "# test[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data size: 2717\n",
            "The most common <e1>audits</e1> were about <e2>waste</e2> and recycling.\n",
            "Message-Topic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuGwk6OwRWGS"
      },
      "source": [
        "Pytorch require the labels to be integers. Create a mapper (dictionary) from the string labels to integers (starting zero). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rKIB5o_vQO8"
      },
      "source": [
        "# Todo: maybe I should use the tagset here, and not the data.\n",
        "def create_label_mapper(data):\n",
        "  counter = 0\n",
        "  tagger = {}\n",
        "  for sentence, tag in data:\n",
        "    if tag not in tagger:\n",
        "      tagger[tag] = counter\n",
        "      counter += 1\n",
        "  \n",
        "  return tagger\n",
        "\n",
        "tagger = create_label_mapper(train)            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wfg_YTeNdzF",
        "outputId": "21d45c86-d48b-4a2f-a422-9f61d917057c"
      },
      "source": [
        "# train_tagger\n",
        "print(tagger)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Component-Whole': 0, 'Other': 1, 'Instrument-Agency': 2, 'Member-Collection': 3, 'Cause-Effect': 4, 'Entity-Destination': 5, 'Content-Container': 6, 'Message-Topic': 7, 'Product-Producer': 8, 'Entity-Origin': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDKYryfKfNdh"
      },
      "source": [
        "**Task 2:** Write a function *prepare_data* that takes one of the [train, test] datasets and convert each pair of (words,labels) to a pair of indexes. The function also aggregate the samples into batches. BERT Uses pretrained tokanization and embedding. you can access the tokanization and indexing using the BertTokenizer class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH-stNKheK66"
      },
      "source": [
        "def prepare_data(data, tokenizer, batch_size=8):\n",
        "    batch = [[], []]\n",
        "\n",
        "    for sample in data:\n",
        "      sentence = sample[0]\n",
        "      tag = sample[1]\n",
        "      batch[0].append(sentence)\n",
        "      batch[1].append(tagger[tag])\n",
        "\n",
        "    max_len = 0\n",
        "    for sent in batch[0]:\n",
        "      input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "      max_len = max(max_len, len(input_ids))\n",
        "\n",
        "    input=[]\n",
        "    attention=[]\n",
        "    tags=batch[1]\n",
        "\n",
        "    for sentence in batch[0]:\n",
        "        encoded_dict = tokenizer(\n",
        "        sentence,\n",
        "        add_special_tokens = True,\n",
        "        padding = \"max_length\",\n",
        "        return_attention_mask = True,\n",
        "        return_tensors = \"pt\",\n",
        "        max_length = max_len\n",
        "        )\n",
        "        \n",
        "        input.append(encoded_dict['input_ids'].to(device))\n",
        "        attention.append(encoded_dict['attention_mask'].to(device))\n",
        "    \n",
        "    input_id=torch.cat(input, dim=0).to(device)\n",
        "    atte=torch.cat(attention, dim=0).to(device)\n",
        "    label_id=torch.tensor(tags).to(device)\n",
        "    dataset = TensorDataset(input_id,atte,label_id)\n",
        "\n",
        "    return DataLoader(\n",
        "      dataset,\n",
        "      sampler=SequentialSampler(dataset),\n",
        "      batch_size=batch_size) ,dataset\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_sequences ,train_dataset = prepare_data(train, tokenizer)\n",
        "test_sequences,test_dataset = prepare_data(test, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSULXl-aIRiG"
      },
      "source": [
        "##split train /Dev\n",
        "def train_dev_dataloader(dataset):\n",
        "\n",
        "# Create a 95-5 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "    train_size = int(0.95 * len(train_dataset))\n",
        "    val_size = len(train_dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "    train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    print('{:>5,} training samples'.format(train_size))\n",
        "    print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "                train_data,  # The training samples.\n",
        "                sampler = RandomSampler(train_data), # Select batches randomly\n",
        "                batch_size =8 # Trains with this batch size.\n",
        "            )\n",
        "\n",
        "    validation_dataloader = DataLoader(\n",
        "                val_data, # The validation samples.\n",
        "                sampler = SequentialSampler(val_data), # Pull out batches sequentially.\n",
        "                batch_size = 8 # Evaluate with this batch size.\n",
        "            )\n",
        "    return train_dataloader,validation_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kujt7q9IlTK",
        "outputId": "f3b34356-7e08-416d-8f7a-b133a63a044f"
      },
      "source": [
        "train_data,validation_dataloader= ret_dataloader(train_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7,600 training samples\n",
            "  400 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEGSQdeUkTP8"
      },
      "source": [
        "**Task 3:** In this part we classify the sentences using the BertForSequenceClassification model. To save resources, we initialize the optimizer with the final layer of the model. You are also allowed to change the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avkHfjT3k0HM"
      },
      "source": [
        "def get_parameters(params):\n",
        "  param_list = list(params)\n",
        "  p = param_list[-2][-1] ##p[-1] is the bias and it's all zeros\n",
        "  return [p]\n",
        "\n",
        "def init_model():\n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "          \"bert-base-uncased\", \n",
        "          num_labels = len(tagger), \n",
        "          output_attentions = False, # Whether the model returns attentions weights.\n",
        "          output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "          )\n",
        "  model.to(device)\n",
        "  return model\n",
        "\n",
        "model = init_model()\n",
        "\n",
        "# Optimizer (ADAM is a fancy version of SGD)\n",
        "optimizer = torch.optim.Adam(get_parameters(model.named_parameters()), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3n4cCb8wpXE"
      },
      "source": [
        "**Task 4:** Write a training loop, which takes a BertForSequenceClassification model and number of epochs to train on. The loss is always CrossEntropyLoss and the optimizer is always Adam. You are allowed to split the train to train and dev sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL9GpZuge1rP"
      },
      "source": [
        "def train_loop(model, n_epochs, train_data, optimizer,comment):\n",
        "  # Loss function\n",
        "  print('Start training:')\n",
        "  criterion = nn.CrossEntropyLoss() # Loss function\n",
        "  train_loss_array=[]\n",
        "  train_loss_e=[]\n",
        "  batch_size = 50\n",
        "  \n",
        "  print(train_data)\n",
        "  \n",
        "  for e in range(1, n_epochs + 1):\n",
        "    train_loss = 0.0\n",
        "    running_loss= 0.0\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(e, n_epochs))\n",
        "    print(comment)\n",
        "\n",
        "    for i , batch in enumerate(train_data):\n",
        "      input_ids = batch[0].to(device)\n",
        "      input_mask = batch[1].to(device)\n",
        "      labels = batch[2].to(device)\n",
        "      model.zero_grad()        \n",
        "      outputs = model(input_ids, token_type_ids=None, attention_mask=input_mask, labels=labels)\n",
        "\n",
        "      loss =  criterion(outputs.logits, labels)\n",
        "\n",
        "      optimizer.zero_grad() # Zero the gradients\n",
        "\n",
        "      loss.backward()   # Perform a backward pass (backpropagation)\n",
        "\n",
        "      optimizer.step()  # Update the parameters\n",
        "      \n",
        "      # statistics\n",
        "      train_loss += loss.item()\n",
        "      running_loss += loss.item()\n",
        "      if i % batch_size == 0 and i > 1 :    \n",
        "        train_loss_array.append(running_loss / batch_size)\n",
        "        print('Batch {:>5,}  of  {:>5,} loss: {:}' .format( i ,len(train_data), running_loss / batch_size))\n",
        "        running_loss = 0.0\n",
        "\n",
        "   # calculate train loss for epoch\n",
        "    train_loss_calc = train_loss / len(train_data)\n",
        "    print('========Epoch {:} Loss {:} ========'.format(e,train_loss_calc))\n",
        "    train_loss_e.append(train_loss_calc)\n",
        "\n",
        "  return model, optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAdQGXQExJ66",
        "outputId": "0a1f99c8-9d6d-48c5-d5a5-761c42e29843"
      },
      "source": [
        "model_trained, optimizer = train_loop(model, 10, train_data, optimizer,\"Train\")\n",
        "model, optimizer = train_loop(model_trained, 10, validation_dataloader, optimizer,\"Dev\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training:\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f5aa16c6390>\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Train\n",
            "Batch    50  of    950 loss: 2.385766966342926\n",
            "Batch   100  of    950 loss: 2.2229510045051573\n",
            "Batch   150  of    950 loss: 2.2323870968818667\n",
            "Batch   200  of    950 loss: 2.1985343623161318\n",
            "Batch   250  of    950 loss: 2.15387024641037\n",
            "Batch   300  of    950 loss: 2.1752386260032655\n",
            "Batch   350  of    950 loss: 2.138786907196045\n",
            "Batch   400  of    950 loss: 2.196463992595673\n",
            "Batch   450  of    950 loss: 2.0697077918052673\n",
            "Batch   500  of    950 loss: 2.104599988460541\n",
            "Batch   550  of    950 loss: 2.086870565414429\n",
            "Batch   600  of    950 loss: 2.0948660874366762\n",
            "Batch   650  of    950 loss: 2.038350718021393\n",
            "Batch   700  of    950 loss: 2.0542921662330627\n",
            "Batch   750  of    950 loss: 2.0359427905082703\n",
            "Batch   800  of    950 loss: 1.999629328250885\n",
            "Batch   850  of    950 loss: 1.9598525333404542\n",
            "Batch   900  of    950 loss: 1.985708978176117\n",
            "========Epoch 1 Loss 2.110216003970096 ========\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Train\n",
            "Batch    50  of    950 loss: 1.985335717201233\n",
            "Batch   100  of    950 loss: 1.9874512672424316\n",
            "Batch   150  of    950 loss: 1.9597544598579406\n",
            "Batch   200  of    950 loss: 1.9923059058189392\n",
            "Batch   250  of    950 loss: 1.962963502407074\n",
            "Batch   300  of    950 loss: 1.8629972505569459\n",
            "Batch   350  of    950 loss: 1.8772619295120239\n",
            "Batch   400  of    950 loss: 1.9481004166603089\n",
            "Batch   450  of    950 loss: 1.9311247777938843\n",
            "Batch   500  of    950 loss: 1.948208019733429\n",
            "Batch   550  of    950 loss: 1.9337122082710265\n",
            "Batch   600  of    950 loss: 1.924831120967865\n",
            "Batch   650  of    950 loss: 1.8983231115341186\n",
            "Batch   700  of    950 loss: 1.8767831110954285\n",
            "Batch   750  of    950 loss: 1.9006433629989623\n",
            "Batch   800  of    950 loss: 1.836885735988617\n",
            "Batch   850  of    950 loss: 1.8610841298103333\n",
            "Batch   900  of    950 loss: 1.810048246383667\n",
            "========Epoch 2 Loss 1.9122770904239856 ========\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Train\n",
            "Batch    50  of    950 loss: 1.9025993204116822\n",
            "Batch   100  of    950 loss: 1.8416471314430236\n",
            "Batch   150  of    950 loss: 1.867126190662384\n",
            "Batch   200  of    950 loss: 1.826233468055725\n",
            "Batch   250  of    950 loss: 1.827736895084381\n",
            "Batch   300  of    950 loss: 1.878987181186676\n",
            "Batch   350  of    950 loss: 1.8842764377593995\n",
            "Batch   400  of    950 loss: 1.8184426760673522\n",
            "Batch   450  of    950 loss: 1.7747933173179626\n",
            "Batch   500  of    950 loss: 1.753269577026367\n",
            "Batch   550  of    950 loss: 1.8409044456481933\n",
            "Batch   600  of    950 loss: 1.792651436328888\n",
            "Batch   650  of    950 loss: 1.7737437987327576\n",
            "Batch   700  of    950 loss: 1.7572614645957947\n",
            "Batch   750  of    950 loss: 1.749352946281433\n",
            "Batch   800  of    950 loss: 1.806360890865326\n",
            "Batch   850  of    950 loss: 1.747910702228546\n",
            "Batch   900  of    950 loss: 1.7999525332450867\n",
            "========Epoch 3 Loss 1.808640639530985 ========\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Train\n",
            "Batch    50  of    950 loss: 1.7494291520118714\n",
            "Batch   100  of    950 loss: 1.7317731404304504\n",
            "Batch   150  of    950 loss: 1.6938608765602112\n",
            "Batch   200  of    950 loss: 1.7905881905555725\n",
            "Batch   250  of    950 loss: 1.7255253458023072\n",
            "Batch   300  of    950 loss: 1.6622034549713134\n",
            "Batch   350  of    950 loss: 1.7476212692260742\n",
            "Batch   400  of    950 loss: 1.7424352955818176\n",
            "Batch   450  of    950 loss: 1.8321318745613098\n",
            "Batch   500  of    950 loss: 1.7382976365089418\n",
            "Batch   550  of    950 loss: 1.8186067962646484\n",
            "Batch   600  of    950 loss: 1.7469697523117065\n",
            "Batch   650  of    950 loss: 1.6700034165382385\n",
            "Batch   700  of    950 loss: 1.772842562198639\n",
            "Batch   750  of    950 loss: 1.6964731001853943\n",
            "Batch   800  of    950 loss: 1.7368019938468933\n",
            "Batch   850  of    950 loss: 1.708021309375763\n",
            "Batch   900  of    950 loss: 1.7416136455535889\n",
            "========Epoch 4 Loss 1.7379468858869451 ========\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Train\n",
            "Batch    50  of    950 loss: 1.740118421316147\n",
            "Batch   100  of    950 loss: 1.6543867802619934\n",
            "Batch   150  of    950 loss: 1.6901288902759553\n",
            "Batch   200  of    950 loss: 1.6894118809700012\n",
            "Batch   250  of    950 loss: 1.729561948776245\n",
            "Batch   300  of    950 loss: 1.675972762107849\n",
            "Batch   350  of    950 loss: 1.6179262566566468\n",
            "Batch   400  of    950 loss: 1.6954009485244752\n",
            "Batch   450  of    950 loss: 1.7549946403503418\n",
            "Batch   500  of    950 loss: 1.7048042249679565\n",
            "Batch   550  of    950 loss: 1.7403358721733093\n",
            "Batch   600  of    950 loss: 1.6282615709304809\n",
            "Batch   650  of    950 loss: 1.6927940082550048\n",
            "Batch   700  of    950 loss: 1.6116297399997712\n",
            "Batch   750  of    950 loss: 1.684441773891449\n",
            "Batch   800  of    950 loss: 1.6436470830440522\n",
            "Batch   850  of    950 loss: 1.7415096807479857\n",
            "Batch   900  of    950 loss: 1.6983760380744934\n",
            "========Epoch 5 Loss 1.6879961342560617 ========\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Train\n",
            "Batch    50  of    950 loss: 1.6850292921066283\n",
            "Batch   100  of    950 loss: 1.6989210152626038\n",
            "Batch   150  of    950 loss: 1.7290387701988221\n",
            "Batch   200  of    950 loss: 1.7334328126907348\n",
            "Batch   250  of    950 loss: 1.6065314149856567\n",
            "Batch   300  of    950 loss: 1.624096586704254\n",
            "Batch   350  of    950 loss: 1.7531759214401246\n",
            "Batch   400  of    950 loss: 1.6912882387638093\n",
            "Batch   450  of    950 loss: 1.6198855102062226\n",
            "Batch   500  of    950 loss: 1.641948093175888\n",
            "Batch   550  of    950 loss: 1.627953871488571\n",
            "Batch   600  of    950 loss: 1.6643079352378845\n",
            "Batch   650  of    950 loss: 1.6288791918754577\n",
            "Batch   700  of    950 loss: 1.6149848890304566\n",
            "Batch   750  of    950 loss: 1.599912898540497\n",
            "Batch   800  of    950 loss: 1.672898063659668\n",
            "Batch   850  of    950 loss: 1.6483135151863098\n",
            "Batch   900  of    950 loss: 1.647286865711212\n",
            "========Epoch 6 Loss 1.6568270939274838 ========\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Train\n",
            "Batch    50  of    950 loss: 1.740010175704956\n",
            "Batch   100  of    950 loss: 1.6179537510871886\n",
            "Batch   150  of    950 loss: 1.5568149924278258\n",
            "Batch   200  of    950 loss: 1.577687782049179\n",
            "Batch   250  of    950 loss: 1.5903980398178101\n",
            "Batch   300  of    950 loss: 1.634111979007721\n",
            "Batch   350  of    950 loss: 1.5736534690856934\n",
            "Batch   400  of    950 loss: 1.5784492564201356\n",
            "Batch   450  of    950 loss: 1.5671048772335052\n",
            "Batch   500  of    950 loss: 1.5716005516052247\n",
            "Batch   550  of    950 loss: 1.6398100805282594\n",
            "Batch   600  of    950 loss: 1.5457160878181457\n",
            "Batch   650  of    950 loss: 1.693487913608551\n",
            "Batch   700  of    950 loss: 1.6842190492153168\n",
            "Batch   750  of    950 loss: 1.6956680071353913\n",
            "Batch   800  of    950 loss: 1.6146679282188416\n",
            "Batch   850  of    950 loss: 1.677125824689865\n",
            "Batch   900  of    950 loss: 1.6245942509174347\n",
            "========Epoch 7 Loss 1.6168599152565002 ========\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Train\n",
            "Batch    50  of    950 loss: 1.6718968188762664\n",
            "Batch   100  of    950 loss: 1.5349671959877014\n",
            "Batch   150  of    950 loss: 1.7283789277076722\n",
            "Batch   200  of    950 loss: 1.5743066000938415\n",
            "Batch   250  of    950 loss: 1.5378692317008973\n",
            "Batch   300  of    950 loss: 1.7109207499027252\n",
            "Batch   350  of    950 loss: 1.5671170771121978\n",
            "Batch   400  of    950 loss: 1.5424623239040374\n",
            "Batch   450  of    950 loss: 1.6052570390701293\n",
            "Batch   500  of    950 loss: 1.6299068665504455\n",
            "Batch   550  of    950 loss: 1.5320090639591217\n",
            "Batch   600  of    950 loss: 1.5921848344802856\n",
            "Batch   650  of    950 loss: 1.5325044894218445\n",
            "Batch   700  of    950 loss: 1.6487466311454773\n",
            "Batch   750  of    950 loss: 1.6109944200515747\n",
            "Batch   800  of    950 loss: 1.6306548738479614\n",
            "Batch   850  of    950 loss: 1.555669218301773\n",
            "Batch   900  of    950 loss: 1.5768512547016145\n",
            "========Epoch 8 Loss 1.5994019202809584 ========\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Train\n",
            "Batch    50  of    950 loss: 1.7003445076942443\n",
            "Batch   100  of    950 loss: 1.5538113558292388\n",
            "Batch   150  of    950 loss: 1.5746578204631805\n",
            "Batch   200  of    950 loss: 1.5986633563041688\n",
            "Batch   250  of    950 loss: 1.6418897593021393\n",
            "Batch   300  of    950 loss: 1.531956444978714\n",
            "Batch   350  of    950 loss: 1.5322770261764527\n",
            "Batch   400  of    950 loss: 1.522548291683197\n",
            "Batch   450  of    950 loss: 1.5831590723991393\n",
            "Batch   500  of    950 loss: 1.451790473461151\n",
            "Batch   550  of    950 loss: 1.539590492248535\n",
            "Batch   600  of    950 loss: 1.5945571887493133\n",
            "Batch   650  of    950 loss: 1.608405854701996\n",
            "Batch   700  of    950 loss: 1.6210799348354339\n",
            "Batch   750  of    950 loss: 1.5946249628067017\n",
            "Batch   800  of    950 loss: 1.6219156622886657\n",
            "Batch   850  of    950 loss: 1.6017499089241027\n",
            "Batch   900  of    950 loss: 1.5172058868408203\n",
            "========Epoch 9 Loss 1.5778105744562652 ========\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Train\n",
            "Batch    50  of    950 loss: 1.5523277497291565\n",
            "Batch   100  of    950 loss: 1.5283707964420319\n",
            "Batch   150  of    950 loss: 1.6431759989261627\n",
            "Batch   200  of    950 loss: 1.496264865398407\n",
            "Batch   250  of    950 loss: 1.4459827625751496\n",
            "Batch   300  of    950 loss: 1.5529960405826568\n",
            "Batch   350  of    950 loss: 1.546116601228714\n",
            "Batch   400  of    950 loss: 1.5711598122119903\n",
            "Batch   450  of    950 loss: 1.5044372940063477\n",
            "Batch   500  of    950 loss: 1.5830203258991242\n",
            "Batch   550  of    950 loss: 1.531158242225647\n",
            "Batch   600  of    950 loss: 1.4515302515029906\n",
            "Batch   650  of    950 loss: 1.557949652671814\n",
            "Batch   700  of    950 loss: 1.6077534019947053\n",
            "Batch   750  of    950 loss: 1.6657653403282167\n",
            "Batch   800  of    950 loss: 1.5842609047889709\n",
            "Batch   850  of    950 loss: 1.5431784415245056\n",
            "Batch   900  of    950 loss: 1.5241139578819274\n",
            "========Epoch 10 Loss 1.5483922544278597 ========\n",
            "Start training:\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f5aa16c6490>\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Dev\n",
            "========Epoch 1 Loss 1.4917406344413757 ========\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Dev\n",
            "========Epoch 2 Loss 1.4513897800445557 ========\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Dev\n",
            "========Epoch 3 Loss 1.4318213057518006 ========\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Dev\n",
            "========Epoch 4 Loss 1.4127984642982483 ========\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Dev\n",
            "========Epoch 5 Loss 1.3954546773433685 ========\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Dev\n",
            "========Epoch 6 Loss 1.3794004535675048 ========\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Dev\n",
            "========Epoch 7 Loss 1.364366239309311 ========\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Dev\n",
            "========Epoch 8 Loss 1.3501700139045716 ========\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Dev\n",
            "========Epoch 9 Loss 1.3366804397106171 ========\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Dev\n",
            "========Epoch 10 Loss 1.323799457550049 ========\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baN1c_B7lTjb"
      },
      "source": [
        "**Task 5:** write an evaluation loop on a trained model, using the dev and test datasets. This function print the true positive rate (TPR), also known as Recall and the opposite to false positive rate (FPR), also known as precision, of each label seperately (10 labels in total), and for all labels together. The caption argument for the function should be served for printing, so that when you print include it as a prefix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyQAjGaqmd8U"
      },
      "source": [
        "def evaluate(model, test_data):\n",
        "  tags= list(tagger.keys())\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  confusion_matrix = torch.zeros(len(tags),len(tags))\n",
        "  with torch.no_grad():\n",
        "    for step , batch in enumerate(test_data):\n",
        "          ##print(\"pass\")\n",
        "          input_ids = batch[0].to(device)\n",
        "          input_mask = batch[1].to(device)\n",
        "          labels = batch[2].to(device)\n",
        "          outputs = model(input_ids, token_type_ids=None, attention_mask=input_mask,return_dict=True)\n",
        "          logits = outputs.logits.to(device)\n",
        "          pred_flat = torch.argmax(logits, dim=1)\n",
        "          pred_flat = pred_flat.cpu().numpy()\n",
        "          labels=labels.cpu().numpy()\n",
        "          \n",
        "          total += len(labels)\n",
        "          correct += (pred_flat == labels).sum()\n",
        "          \n",
        "          for i in range(len(labels)):\n",
        "                  confusion_matrix[pred_flat[i],labels[i]] += 1\n",
        "  \n",
        "  acc=correct / total\n",
        "        \n",
        "  return confusion_matrix,acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drurSasVRCx_"
      },
      "source": [
        "confusion_matrix,acc = evaluate(model,test_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNGXqXCTSlZX",
        "outputId": "008be72e-1ea1-4ac0-e2f7-9d154557fb15"
      },
      "source": [
        "print(\"The Accuracy is:\" , acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Accuracy is: 0.43172616856827384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "7m8OhCpia4MH",
        "outputId": "b9f8f969-b6a8-4ec5-c8ea-e0978e405d41"
      },
      "source": [
        "##printing \n",
        "confusion_matrix=confusion_matrix.numpy()\n",
        "tags= list(tagger.keys())\n",
        "indexes= tags+[\"Total\"]\n",
        "num_of_tags=10\n",
        "df = pd.DataFrame(data=np.zeros((num_of_tags+1,2)), index= indexes, columns=[\"precision\",\"recall\"])\n",
        "TP_total ,FN_total,FP_total =0.0 ,0.0 ,0.0 \n",
        "TP,FN,FP=0.0,0.0,0.0\n",
        "for i in range(len(tags)):\n",
        "\n",
        "    TP=confusion_matrix[i,i]\n",
        "    FN=np.sum(confusion_matrix[:,i])-confusion_matrix[i,i]\n",
        "    FP=np.sum(confusion_matrix[i,:])-confusion_matrix[i,i]\n",
        "\n",
        "    if TP ==0 and FN ==0:\n",
        "        recall=0.0\n",
        "    recall = TP / (TP+FN)\n",
        "    if TP ==0 and FP ==0:\n",
        "        precision=0.0\n",
        "    precision = TP / (TP+FP)\n",
        "    \n",
        "    df.loc[tags[i]]=[float(precision),float(recall)]\n",
        "\n",
        "    TP_total += TP\n",
        "    FN_total += FN\n",
        "    FP_total += FP\n",
        "\n",
        "total_recall=TP_total/(TP_total+FN_total)\n",
        "total_precision=TP_total/(TP_total+FP_total)\n",
        "df.loc[indexes[10]]=[float(total_precision),float(total_recall)]\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Component-Whole</th>\n",
              "      <td>0.662162</td>\n",
              "      <td>0.314103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Other</th>\n",
              "      <td>0.230975</td>\n",
              "      <td>0.381057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Instrument-Agency</th>\n",
              "      <td>0.302326</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Member-Collection</th>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.364807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Cause-Effect</th>\n",
              "      <td>0.712281</td>\n",
              "      <td>0.618902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Entity-Destination</th>\n",
              "      <td>0.466200</td>\n",
              "      <td>0.684932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Content-Container</th>\n",
              "      <td>0.795699</td>\n",
              "      <td>0.385417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Message-Topic</th>\n",
              "      <td>0.585366</td>\n",
              "      <td>0.459770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Product-Producer</th>\n",
              "      <td>0.337778</td>\n",
              "      <td>0.329004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Entity-Origin</th>\n",
              "      <td>0.352490</td>\n",
              "      <td>0.356589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Total</th>\n",
              "      <td>0.431726</td>\n",
              "      <td>0.431726</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    precision    recall\n",
              "Component-Whole      0.662162  0.314103\n",
              "Other                0.230975  0.381057\n",
              "Instrument-Agency    0.302326  0.333333\n",
              "Member-Collection    0.566667  0.364807\n",
              "Cause-Effect         0.712281  0.618902\n",
              "Entity-Destination   0.466200  0.684932\n",
              "Content-Container    0.795699  0.385417\n",
              "Message-Topic        0.585366  0.459770\n",
              "Product-Producer     0.337778  0.329004\n",
              "Entity-Origin        0.352490  0.356589\n",
              "Total                0.431726  0.431726"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjkeUfncUl42"
      },
      "source": [
        "**Task 6:** In this part we'll improve the model accuracy by using a method called \"entity markers - Entity start\". The main idea of this approch is to add special markers ([e1], [\\e1], ...) before and after each of the tagged entities. instead of using the CLS token for clasification, we will use the concatination of the embedding of [e1] and [e2] as shown in the image below. The complete method is described in details in the following paper - https://arxiv.org/pdf/1906.03158.pdf (specifically in Section 3.2). To use this method we'll need to create a new data load and a new model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKhIbJuzc_EE"
      },
      "source": [
        "![Capture.PNG](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTRk1T81VoXj"
      },
      "source": [
        "The new methods require the usage of special tokens. The following code will add the required tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU8jayVXYGsK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b7fe8c3-055a-443e-8bb4-3f87efd75aa9"
      },
      "source": [
        "tokenizer.add_tokens(['<e1>', '</e1>', '<e2>', '</e2>'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2UMaxjFYOB7"
      },
      "source": [
        "Create a new dataloader that add entity markers to the dataset and return their indexes as part of the new sample (the expected sample should be (s, l, i) where s is the sentence embedding, l is the label, and i is a touple with the indexes of the start entities)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHM8zgJfaUkf"
      },
      "source": [
        "def get_max_len(sentences):\n",
        "    max_len = 0\n",
        "    for sentence in sentences:\n",
        "      input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
        "      max_len = max(max_len, len(input_ids))\n",
        "    return max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMMMR1kSYnc3"
      },
      "source": [
        "def prepare_data_MTB(data, tokenizer, batch_size = 8):\n",
        "    batch = [[], []]\n",
        "    input=[]\n",
        "    attention=[]\n",
        "    tags=[]\n",
        "    markers_indices = []\n",
        "    \n",
        "    for sample in data:\n",
        "      sentence = sample[0]\n",
        "      tag = sample[1]\n",
        "      batch[0].append(sentence)\n",
        "      batch[1].append(tagger[tag])\n",
        "    \n",
        "    max_len = get_max_len(batch[0])\n",
        "    vocab = tokenizer.get_vocab()\n",
        "\n",
        "    for sentence, tag in zip(batch[0], batch[1]):\n",
        "      encoded_dict = tokenizer(\n",
        "        sentence,\n",
        "        add_special_tokens = True,\n",
        "        padding = \"max_length\",\n",
        "        return_attention_mask = True,\n",
        "        return_tensors = \"pt\",\n",
        "        max_length = max_len\n",
        "      )\n",
        "      \n",
        "      input.append(encoded_dict['input_ids'].to(device))\n",
        "      attention.append(encoded_dict['attention_mask'].to(device))\n",
        "      tags.append(tag)\n",
        "      markers_indices.append([encoded_dict['input_ids'].tolist()[0].index(id) for id in [vocab['<e1>'], vocab['<e2>']]])\n",
        "\n",
        "    dataset = TensorDataset(\n",
        "        torch.cat(input, dim=0).to(device),\n",
        "        torch.cat(attention, dim=0).to(device),\n",
        "        torch.tensor(tags).to(device),\n",
        "        torch.tensor(markers_indices).to(device)\n",
        "    )\n",
        "\n",
        "    return DataLoader(\n",
        "        dataset = dataset,\n",
        "        sampler = SequentialSampler(dataset),\n",
        "        batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIjKOz4iaQs6"
      },
      "source": [
        "train_sequences = prepare_data_MTB(train, tokenizer)\n",
        "test_sequences = prepare_data_MTB(test, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AztAwecuYyt3"
      },
      "source": [
        "Create a new model that uses the \"entity markers - Entity start\" method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADpFlxPFY5mD"
      },
      "source": [
        "class MTB(nn.Module):\n",
        "    def __init__(self, base_model_name):\n",
        "      super().__init__()\n",
        "      self.hidden_size = 768 # bert's default.\n",
        "      self.model = init_model()\n",
        "      self.model.classifier = nn.Linear(self.hidden_size, len(tagger))\n",
        "      # https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
        "      # self.model.classifier.bias.data.fill_(0.01) \n",
        "      self.model.resize_token_embeddings(len(tokenizer))\n",
        "      self.max_pool = nn.MaxPool1d(kernel_size = 3, stride = 2, padding = 1)\n",
        "      self.to(device)\n",
        "\n",
        "    def forward(self, input, index, attention_mask, labels):\n",
        "      bert = self.model.bert(\n",
        "          input,\n",
        "          attention_mask = attention_mask\n",
        "          )\n",
        "      maxpool = self.max_pool(bert[0])\n",
        "      batch_size = input.shape[0]\n",
        "      # make two layers from the original size\n",
        "      tensor1 = torch.zeros((batch_size, int(self.hidden_size/2))).to(device)\n",
        "      tensor2 = torch.zeros((batch_size, int(self.hidden_size/2))).to(device)\n",
        "      \n",
        "      for idx, seq in enumerate(maxpool):\n",
        "        tensor1[idx] = seq[index[:,0][idx]]\n",
        "        tensor2[idx] = seq[index[:,1][idx]]\n",
        "\n",
        "      logits_layer = self.model.classifier(torch.cat((tensor1, tensor2), 1))\n",
        "      loss = nn.CrossEntropyLoss()\n",
        "      loss = loss(logits_layer.view(-1, self.model.num_labels), labels.view(-1))\n",
        "      \n",
        "      return SequenceClassifierOutput(\n",
        "          loss = loss,\n",
        "          logits = logits_layer,\n",
        "          hidden_states = bert.hidden_states,\n",
        "          attentions = bert.attentions\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QKTpKtGajoH"
      },
      "source": [
        "model_mtb = MTB('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3PQYxqcap_Y"
      },
      "source": [
        "def train_loop_MTB(model, n_epochs, train_data, optimizer, is_mtb = False):\n",
        "  # Loss function\n",
        "  print('Start training:')\n",
        "  criterion = nn.CrossEntropyLoss() # Loss function\n",
        "  train_loss_array=[]\n",
        "  train_loss_e=[]\n",
        "  batch_size = 50\n",
        "  \n",
        "  print(train_data)\n",
        "  \n",
        "  for e in range(1, n_epochs + 1):\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(e, n_epochs))\n",
        "    print('Training...')\n",
        "    train_loss = 0.0\n",
        "    running_loss= 0.0\n",
        "    # model.train()\n",
        "\n",
        "    for i, batch in enumerate(train_data):\n",
        "      input_ids = batch[0].to(device)\n",
        "      input_mask = batch[1].to(device)\n",
        "      labels = batch[2].to(device)\n",
        "      # model.zero_grad()\n",
        "      \n",
        "      if is_mtb:\n",
        "        # self, input, index, attention, labels):\n",
        "        outputs = model(\n",
        "            input_ids,\n",
        "            index = batch[3].to(device),\n",
        "            attention_mask = input_mask,\n",
        "            labels = labels)\n",
        "      else:\n",
        "        outputs = model(\n",
        "            input_ids, \n",
        "            attention_mask = input_mask,\n",
        "            token_type_ids = None, \n",
        "            labels=labels)\n",
        "\n",
        "      loss =  criterion(outputs.logits, labels)\n",
        "      \n",
        "      optimizer.zero_grad() # Zero the gradients\n",
        "      loss.backward()   # Perform a backward pass (backpropagation)\n",
        "      optimizer.step()  # Update the parameters\n",
        "\n",
        "      # statistics\n",
        "      train_loss += loss.item()\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      if i % batch_size == 0 and i > 1 :    \n",
        "        train_loss_array.append(running_loss / batch_size)\n",
        "        print('Batch {:>5,}  of  {:>5,} loss: {:}' .format( i ,len(train_data), running_loss / batch_size))\n",
        "        running_loss = 0.0\n",
        "\n",
        "   # calculate train loss for epoch\n",
        "    train_loss_calc = train_loss / len(train_data)\n",
        "    print('========Epoch {:} Loss {:} ========'.format(e,train_loss_calc))\n",
        "    train_loss_e.append(train_loss_calc)\n",
        "\n",
        "  return model, optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5uVAV20anMP",
        "outputId": "40ba8179-1ec4-4c09-f5b9-72dada8978e3"
      },
      "source": [
        "model_trained_mtb, optimizer = train_loop_MTB(model_mtb, 5, train_sequences, optimizer, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training:\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f5aa2b81ed0>\n",
            "\n",
            "======== Epoch 1 / 5 ========\n",
            "Training...\n",
            "Batch    50  of  1,000 loss: 2.373382611274719\n",
            "Batch   100  of  1,000 loss: 2.3302900695800783\n",
            "Batch   150  of  1,000 loss: 2.3285653495788576\n",
            "Batch   200  of  1,000 loss: 2.3093593740463256\n",
            "Batch   250  of  1,000 loss: 2.3300725412368775\n",
            "Batch   300  of  1,000 loss: 2.332556757926941\n",
            "Batch   350  of  1,000 loss: 2.33349326133728\n",
            "Batch   400  of  1,000 loss: 2.3182434701919554\n",
            "Batch   450  of  1,000 loss: 2.338885130882263\n",
            "Batch   500  of  1,000 loss: 2.336110916137695\n",
            "Batch   550  of  1,000 loss: 2.338385796546936\n",
            "Batch   600  of  1,000 loss: 2.327881345748901\n",
            "Batch   650  of  1,000 loss: 2.34012291431427\n",
            "Batch   700  of  1,000 loss: 2.3287598657608033\n",
            "Batch   750  of  1,000 loss: 2.3359047079086306\n",
            "Batch   800  of  1,000 loss: 2.3073558712005617\n",
            "Batch   850  of  1,000 loss: 2.3370355272293093\n",
            "Batch   900  of  1,000 loss: 2.2999488258361818\n",
            "Batch   950  of  1,000 loss: 2.2754599046707153\n",
            "========Epoch 1 Loss 2.3216874792575837 ========\n",
            "\n",
            "======== Epoch 2 / 5 ========\n",
            "Training...\n",
            "Batch    50  of  1,000 loss: 2.373382611274719\n",
            "Batch   100  of  1,000 loss: 2.3302900695800783\n",
            "Batch   150  of  1,000 loss: 2.3285653495788576\n",
            "Batch   200  of  1,000 loss: 2.3093593740463256\n",
            "Batch   250  of  1,000 loss: 2.3300725412368775\n",
            "Batch   300  of  1,000 loss: 2.332556757926941\n",
            "Batch   350  of  1,000 loss: 2.33349326133728\n",
            "Batch   400  of  1,000 loss: 2.3182434701919554\n",
            "Batch   450  of  1,000 loss: 2.338885130882263\n",
            "Batch   500  of  1,000 loss: 2.336110916137695\n",
            "Batch   550  of  1,000 loss: 2.338385796546936\n",
            "Batch   600  of  1,000 loss: 2.327881345748901\n",
            "Batch   650  of  1,000 loss: 2.34012291431427\n",
            "Batch   700  of  1,000 loss: 2.3287598657608033\n",
            "Batch   750  of  1,000 loss: 2.3359047079086306\n",
            "Batch   800  of  1,000 loss: 2.3073558712005617\n",
            "Batch   850  of  1,000 loss: 2.3370355272293093\n",
            "Batch   900  of  1,000 loss: 2.2999488258361818\n",
            "Batch   950  of  1,000 loss: 2.2754599046707153\n",
            "========Epoch 2 Loss 2.3216874792575837 ========\n",
            "\n",
            "======== Epoch 3 / 5 ========\n",
            "Training...\n",
            "Batch    50  of  1,000 loss: 2.373382611274719\n",
            "Batch   100  of  1,000 loss: 2.3302900695800783\n",
            "Batch   150  of  1,000 loss: 2.3285653495788576\n",
            "Batch   200  of  1,000 loss: 2.3093593740463256\n",
            "Batch   250  of  1,000 loss: 2.3300725412368775\n",
            "Batch   300  of  1,000 loss: 2.332556757926941\n",
            "Batch   350  of  1,000 loss: 2.33349326133728\n",
            "Batch   400  of  1,000 loss: 2.3182434701919554\n",
            "Batch   450  of  1,000 loss: 2.338885130882263\n",
            "Batch   500  of  1,000 loss: 2.336110916137695\n",
            "Batch   550  of  1,000 loss: 2.338385796546936\n",
            "Batch   600  of  1,000 loss: 2.327881345748901\n",
            "Batch   650  of  1,000 loss: 2.34012291431427\n",
            "Batch   700  of  1,000 loss: 2.3287598657608033\n",
            "Batch   750  of  1,000 loss: 2.3359047079086306\n",
            "Batch   800  of  1,000 loss: 2.3073558712005617\n",
            "Batch   850  of  1,000 loss: 2.3370355272293093\n",
            "Batch   900  of  1,000 loss: 2.2999488258361818\n",
            "Batch   950  of  1,000 loss: 2.2754599046707153\n",
            "========Epoch 3 Loss 2.3216874792575837 ========\n",
            "\n",
            "======== Epoch 4 / 5 ========\n",
            "Training...\n",
            "Batch    50  of  1,000 loss: 2.373382611274719\n",
            "Batch   100  of  1,000 loss: 2.3302900695800783\n",
            "Batch   150  of  1,000 loss: 2.3285653495788576\n",
            "Batch   200  of  1,000 loss: 2.3093593740463256\n",
            "Batch   250  of  1,000 loss: 2.3300725412368775\n",
            "Batch   300  of  1,000 loss: 2.332556757926941\n",
            "Batch   350  of  1,000 loss: 2.33349326133728\n",
            "Batch   400  of  1,000 loss: 2.3182434701919554\n",
            "Batch   450  of  1,000 loss: 2.338885130882263\n",
            "Batch   500  of  1,000 loss: 2.336110916137695\n",
            "Batch   550  of  1,000 loss: 2.338385796546936\n",
            "Batch   600  of  1,000 loss: 2.327881345748901\n",
            "Batch   650  of  1,000 loss: 2.34012291431427\n",
            "Batch   700  of  1,000 loss: 2.3287598657608033\n",
            "Batch   750  of  1,000 loss: 2.3359047079086306\n",
            "Batch   800  of  1,000 loss: 2.3073558712005617\n",
            "Batch   850  of  1,000 loss: 2.3370355272293093\n",
            "Batch   900  of  1,000 loss: 2.2999488258361818\n",
            "Batch   950  of  1,000 loss: 2.2754599046707153\n",
            "========Epoch 4 Loss 2.3216874792575837 ========\n",
            "\n",
            "======== Epoch 5 / 5 ========\n",
            "Training...\n",
            "Batch    50  of  1,000 loss: 2.373382611274719\n",
            "Batch   100  of  1,000 loss: 2.3302900695800783\n",
            "Batch   150  of  1,000 loss: 2.3285653495788576\n",
            "Batch   200  of  1,000 loss: 2.3093593740463256\n",
            "Batch   250  of  1,000 loss: 2.3300725412368775\n",
            "Batch   300  of  1,000 loss: 2.332556757926941\n",
            "Batch   350  of  1,000 loss: 2.33349326133728\n",
            "Batch   400  of  1,000 loss: 2.3182434701919554\n",
            "Batch   450  of  1,000 loss: 2.338885130882263\n",
            "Batch   500  of  1,000 loss: 2.336110916137695\n",
            "Batch   550  of  1,000 loss: 2.338385796546936\n",
            "Batch   600  of  1,000 loss: 2.327881345748901\n",
            "Batch   650  of  1,000 loss: 2.34012291431427\n",
            "Batch   700  of  1,000 loss: 2.3287598657608033\n",
            "Batch   750  of  1,000 loss: 2.3359047079086306\n",
            "Batch   800  of  1,000 loss: 2.3073558712005617\n",
            "Batch   850  of  1,000 loss: 2.3370355272293093\n",
            "Batch   900  of  1,000 loss: 2.2999488258361818\n",
            "Batch   950  of  1,000 loss: 2.2754599046707153\n",
            "========Epoch 5 Loss 2.3216874792575837 ========\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuwCczHeZjaw"
      },
      "source": [
        "Use the new dataloader and model to train and evaluate the new model as in task 4 and 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A51WDyGUa38k"
      },
      "source": [
        "def evaluate_MTB(model, test_data, is_mtb = True):\n",
        "  tags = list(tagger.keys())\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  confusion_matrix = torch.zeros(len(tags),len(tags))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for step, batch in enumerate(test_data):\n",
        "      input_ids = batch[0].to(device)\n",
        "      input_mask = batch[1].to(device)\n",
        "      labels = batch[2].to(device)\n",
        "      \n",
        "      if is_mtb:\n",
        "        outputs = model(\n",
        "            input_ids,\n",
        "            index = batch[3].to(device),\n",
        "            attention_mask = input_mask,\n",
        "            labels = labels)\n",
        "      else:\n",
        "        outputs = model(\n",
        "            input_ids, \n",
        "            token_type_ids = None, \n",
        "            attention_mask = input_mask)\n",
        "        \n",
        "      logits = outputs.logits.to(device)\n",
        "      pred_flat = torch.argmax(logits, dim=1) # should we add a softmax here?\n",
        "      pred_flat = pred_flat.cpu().numpy()\n",
        "      labels=labels.cpu().numpy()\n",
        "      \n",
        "      total += len(labels)\n",
        "      correct += (pred_flat == labels).sum()\n",
        "      \n",
        "    for i in range(len(labels)):\n",
        "      confusion_matrix[pred_flat[i],labels[i]] += 1\n",
        "  \n",
        "    acc = correct / total\n",
        "          \n",
        "  return confusion_matrix, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NPf1N_Aa6k3"
      },
      "source": [
        "confusion_matrix, acc = evaluate_MTB(model_trained_mtb, test_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klxzTIdNfHaG",
        "outputId": "0a418a9f-299b-4a53-8e1c-32fb9360e7a7"
      },
      "source": [
        "print(\"The Accuracy is:\" , acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Accuracy is: 0.09017298490982702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxaESRoco6bV"
      },
      "source": [
        "**Good luck!**"
      ]
    }
  ]
}